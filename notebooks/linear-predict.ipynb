{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T01:18:19.930074Z",
     "iopub.status.busy": "2025-06-13T01:18:19.929701Z",
     "iopub.status.idle": "2025-06-13T01:18:23.643945Z",
     "shell.execute_reply": "2025-06-13T01:18:23.641621Z",
     "shell.execute_reply.started": "2025-06-13T01:18:19.930044Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:34: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:34: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\ARKO BERA\\AppData\\Local\\Temp\\ipykernel_24424\\3953594468.py:34: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  combined['Title'] = combined['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
      "2025-06-19 19:32:28,107 - INFO - Reading training and test data...\n",
      "2025-06-19 19:32:28,112 - INFO - Combining datasets for feature engineering...\n",
      "2025-06-19 19:32:28,117 - INFO - Extracting titles from names...\n",
      "2025-06-19 19:32:28,120 - INFO - Creating family size and travel-alone features...\n",
      "2025-06-19 19:32:28,122 - INFO - Extracting cabin class...\n",
      "2025-06-19 19:32:28,125 - INFO - Filling missing Fare values and engineering FarePerPerson...\n",
      "2025-06-19 19:32:28,130 - INFO - Filling missing Age values with grouped medians...\n",
      "2025-06-19 19:32:28,372 - INFO - Creating AgeGroup bins...\n",
      "2025-06-19 19:32:28,373 - INFO - Filling missing Embarked values...\n",
      "2025-06-19 19:32:28,375 - INFO - One-hot encoding categorical features...\n",
      "2025-06-19 19:32:28,378 - INFO - Selecting final features...\n",
      "2025-06-19 19:32:28,379 - INFO - Splitting combined data into train and test sets...\n",
      "2025-06-19 19:32:28,380 - INFO - Preparing feature matrix and target variable...\n",
      "2025-06-19 19:32:28,382 - INFO - Creating logistic regression pipeline with standard scaling...\n",
      "2025-06-19 19:32:29,211 - INFO - HTTP Request: GET https://dagshub.com/api/v1/user \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as arkobera\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as arkobera\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 19:32:29,215 - INFO - Accessing as arkobera\n",
      "2025-06-19 19:32:29,844 - INFO - HTTP Request: GET https://dagshub.com/api/v1/repos/arkobera/Titanic_Survival_Prediction \"HTTP/1.1 200 OK\"\n",
      "2025-06-19 19:32:30,419 - INFO - HTTP Request: GET https://dagshub.com/api/v1/user \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"arkobera/Titanic_Survival_Prediction\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"arkobera/Titanic_Survival_Prediction\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 19:32:30,426 - INFO - Initialized MLflow to track repo \"arkobera/Titanic_Survival_Prediction\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository arkobera/Titanic_Survival_Prediction initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository arkobera/Titanic_Survival_Prediction initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 19:32:30,427 - INFO - Repository arkobera/Titanic_Survival_Prediction initialized!\n",
      "2025/06/19 19:32:31 INFO mlflow.tracking.fluent: Experiment with name 'Logistic Regression Grid Search' does not exist. Creating a new experiment.\n",
      "2025-06-19 19:32:32,037 - INFO - Starting logistic regression training with preprocessing\n",
      "2025-06-19 19:32:34,402 - INFO - Best Parameters: {'logisticregression__C': 1, 'logisticregression__class_weight': None, 'logisticregression__penalty': 'l2', 'logisticregression__solver': 'liblinear'}\n",
      "2025-06-19 19:32:34,404 - INFO - Best Cross-Validation Score: 0.8216\n",
      "2025/06/19 19:32:40 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025-06-19 19:32:44,341 - INFO - Pipeline completed and submission saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run logreg_pipeline_run at: https://dagshub.com/arkobera/Titanic_Survival_Prediction.mlflow/#/experiments/2/runs/d21c64a5795546ce83f0fbb5ee2258a6\n",
      "🧪 View experiment at: https://dagshub.com/arkobera/Titanic_Survival_Prediction.mlflow/#/experiments/2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import logging\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import dagshub\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    train_path = \"C:/Users/ARKO BERA/OneDrive/Desktop/MLOPS/Titanic_Survival_Prediction/data/train.csv\"\n",
    "    test_path = \"C:/Users/ARKO BERA/OneDrive/Desktop/MLOPS/Titanic_Survival_Prediction/data/test.csv\"\n",
    "    sub_path = \"C:/Users/ARKO BERA/OneDrive/Desktop/MLOPS/Titanic_Survival_Prediction/data/gender_submission.csv\"\n",
    "    target = \"Survived\"\n",
    "    drop_col = [\"Name\",\"Ticket\",\"PassengerId\"]\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "logger.info(\"Reading training and test data...\")\n",
    "data = pd.read_csv(cfg.train_path)\n",
    "test_data = pd.read_csv(cfg.test_path)\n",
    "\n",
    "logger.info(\"Combining datasets for feature engineering...\")\n",
    "combined = pd.concat([data, test_data], axis=0, ignore_index=True)\n",
    "combined['IsTest'] = combined['Survived'].isnull()\n",
    "\n",
    "logger.info(\"Extracting titles from names...\")\n",
    "combined['Title'] = combined['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "combined['Title'] = combined['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "combined['Title'] = combined['Title'].replace(['Mlle', 'Ms'], 'Miss')\n",
    "combined['Title'] = combined['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "logger.info(\"Creating family size and travel-alone features...\")\n",
    "combined['FamilySize'] = combined['SibSp'] + combined['Parch'] + 1\n",
    "combined['IsAlone'] = 0\n",
    "combined.loc[combined['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "\n",
    "logger.info(\"Extracting cabin class...\")\n",
    "combined['Cabin'] = combined['Cabin'].fillna('X')\n",
    "combined['CabinClass'] = combined['Cabin'].str[0]\n",
    "\n",
    "logger.info(\"Filling missing Fare values and engineering FarePerPerson...\")\n",
    "combined['Fare'] = combined.groupby('Pclass')['Fare'].transform(lambda x: x.fillna(x.median()))\n",
    "combined['FarePerPerson'] = combined['Fare'] / combined['FamilySize']\n",
    "\n",
    "logger.info(\"Filling missing Age values with grouped medians...\")\n",
    "age_map = combined.groupby(['Title', 'Pclass', 'Sex'])['Age'].median().reset_index()\n",
    "for idx in combined[combined['Age'].isnull()].index:\n",
    "    title = combined.loc[idx, 'Title']\n",
    "    pclass = combined.loc[idx, 'Pclass']\n",
    "    sex = combined.loc[idx, 'Sex']\n",
    "    \n",
    "    mask = (age_map['Title'] == title) & (age_map['Pclass'] == pclass) & (age_map['Sex'] == sex)\n",
    "    if age_map[mask].shape[0] > 0:\n",
    "        pred_age = age_map[mask]['Age'].values[0]\n",
    "    else:\n",
    "        pred_age = combined['Age'].median()\n",
    "    \n",
    "    combined.loc[idx, 'Age'] = pred_age\n",
    "\n",
    "logger.info(\"Creating AgeGroup bins...\")\n",
    "combined['AgeGroup'] = pd.cut(combined['Age'], bins=[0, 12, 18, 30, 50, 100], labels=['Child', 'Teen', 'YoungAdult', 'Adult', 'Senior'])\n",
    "\n",
    "logger.info(\"Filling missing Embarked values...\")\n",
    "combined['Embarked'] = combined['Embarked'].fillna(combined['Embarked'].mode()[0])\n",
    "\n",
    "logger.info(\"One-hot encoding categorical features...\")\n",
    "categorical_features = ['Title', 'CabinClass', 'AgeGroup', 'Sex', 'Embarked']\n",
    "combined = pd.get_dummies(combined, columns=categorical_features)\n",
    "\n",
    "logger.info(\"Selecting final features...\")\n",
    "selected_features = [\n",
    "    'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'FarePerPerson', \n",
    "    'FamilySize', 'IsAlone', 'Title_Master', 'Title_Miss', 'Title_Mr', \n",
    "    'Title_Mrs', 'Title_Rare', 'CabinClass_A', 'CabinClass_B', 'CabinClass_C', \n",
    "    'CabinClass_D', 'CabinClass_E', 'CabinClass_F', 'CabinClass_G', 'CabinClass_T', \n",
    "    'CabinClass_X', 'AgeGroup_Child', 'AgeGroup_Teen', 'AgeGroup_YoungAdult', \n",
    "    'AgeGroup_Adult', 'AgeGroup_Senior', 'Sex_female', 'Sex_male', \n",
    "    'Embarked_C', 'Embarked_Q', 'Embarked_S'\n",
    "]\n",
    "\n",
    "logger.info(\"Splitting combined data into train and test sets...\")\n",
    "train_data = combined[~combined['IsTest']].copy()\n",
    "test_data = combined[combined['IsTest']].copy()\n",
    "\n",
    "logger.info(\"Preparing feature matrix and target variable...\")\n",
    "X = train_data[selected_features]\n",
    "y = train_data['Survived'].astype(int)\n",
    "X_test = test_data[selected_features]\n",
    "\n",
    "logger.info(\"Creating logistic regression pipeline with standard scaling...\")\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(random_state=42, max_iter=1000)\n",
    ")\n",
    "\n",
    "\n",
    "# 模型调优 - 简化参数网格以加快训练速度\n",
    "param_grid = {\n",
    "    'logisticregression__C': [1, 10],\n",
    "    'logisticregression__penalty': ['l2'],  # 简化为只使用l2惩罚\n",
    "    'logisticregression__solver': ['liblinear'],  # 简化为只使用liblinear求解器\n",
    "    'logisticregression__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "\n",
    "mlflow.set_experiment(\"Logistic Regression Grid Search\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"logreg_pipeline_run\"):\n",
    "    logger.info(\"Starting logistic regression training with preprocessing\")\n",
    "\n",
    "    # Fitting GridSearchCV\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_score = grid_search.best_score_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    logger.info(f\"Best Parameters: {best_params}\")\n",
    "    logger.info(f\"Best Cross-Validation Score: {best_score:.4f}\")\n",
    "\n",
    "    # Log best parameters manually\n",
    "    for param, value in best_params.items():\n",
    "        mlflow.log_param(param, value)\n",
    "\n",
    "    mlflow.log_metric(\"cv_accuracy\", best_score)\n",
    "\n",
    "    # Optionally log the model\n",
    "    mlflow.sklearn.log_model(best_model, \"logreg_model\")\n",
    "\n",
    "    # Make predictions and save submission\n",
    "    predictions = best_model.predict(X_test).astype(int)\n",
    "    submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_data[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "    submission_path = \"submission_logreg.csv\"\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    mlflow.log_artifact(submission_path)\n",
    "\n",
    "    logger.info(\"Pipeline completed and submission saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 26502,
     "sourceId": 3136,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30527,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
